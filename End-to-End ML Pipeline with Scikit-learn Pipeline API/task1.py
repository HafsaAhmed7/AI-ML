# -*- coding: utf-8 -*-
"""TASK1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18PTb_perCAybVR1eel_nTmHjbR8n3Y09

# End-to-End ML Pipeline with Scikit-learn Pipeline API

## üéØ Project Objective

The primary objective of this project is to construct a robust, reusable, and production-ready machine learning pipeline for predicting customer churn using the Telco Churn Dataset. This will involve integrating various data preprocessing steps with model training and hyperparameter tuning into a single, cohesive workflow.

## üöÄ Key Goals

To achieve our objective, we will focus on the following key goals:

1.  **Data Preprocessing Pipeline:** Implement efficient and scalable data preprocessing steps, including feature scaling and categorical encoding, leveraging Scikit-learn's `Pipeline` API.
2.  **Model Training Integration:** Integrate diverse machine learning models, such as Logistic Regression and Random Forest, directly into the pipeline for streamlined training.
3.  **Hyperparameter Optimization:** Utilize `GridSearchCV` to systematically tune the hyperparameters of the chosen models within the pipeline, identifying the optimal configurations.
4.  **Pipeline Persistency:** Export the complete, trained machine learning pipeline using `joblib` for future reusability and deployment in a production environment.
5.  **Skill Development:** Reinforce practical skills in ML pipeline construction, advanced hyperparameter tuning techniques, and model serialization for deployment readiness.


You can access the dataset used for this project here: [Telco Customer Churn IBM Dataset](https://www.kaggle.com/datasets/yeanzc/telco-customer-churn-ibm-dataset?resource=download)
"""

from google.colab import files
uploaded = files.upload()

"""# Telco Churn Prediction ‚Äì Boosting Recall for Class 1 (Churn) with Logistic Regression and Random Forest"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import joblib

"""# Load dataset"""

df = pd.read_excel('Telco_customer_churn.xlsx')

"""# Drop unnecessary columns"""

drop_cols = ['CustomerID', 'Count', 'Country', 'State', 'City', 'Zip Code', 'Lat Long',
             'Churn Label', 'Churn Reason', 'Churn Score']
df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)

"""# Convert Total Charges to numeric"""

df['Total Charges'] = pd.to_numeric(df['Total Charges'], errors='coerce')
df['Total Charges'].fillna(df['Total Charges'].median(), inplace=True)

"""# Target and features"""

y = df['Churn Value']
X = df.drop(['Churn Value'], axis=1)

"""# Split dataset"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

"""# Column types"""

num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
cat_cols = X.select_dtypes(include=['object']).columns.tolist()

"""# Preprocessors"""

num_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

cat_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer([
    ('num', num_transformer, num_cols),
    ('cat', cat_transformer, cat_cols)
])

"""# Logistic Regression pipeline (with class_weight)"""

logreg_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(class_weight='balanced', solver='liblinear'))
])

# Random Forest pipeline
rf_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42))
])

"""# Hyperparameters"""

logreg_params = {'classifier__C': [0.1, 1, 10]}
rf_params = {'classifier__n_estimators': [100, 200], 'classifier__max_depth': [None, 10, 20]}

# GridSearchCV
grid_logreg = GridSearchCV(logreg_pipeline, logreg_params, cv=5, scoring='recall')
grid_rf = GridSearchCV(rf_pipeline, rf_params, cv=5, scoring='recall')

"""# Fit both models"""

grid_logreg.fit(X_train, y_train)
grid_rf.fit(X_train, y_train)

"""# Evaluate both"""

print("üìå Logistic Regression Results:")
logreg_pred = grid_logreg.predict(X_test)
print("Best Params:", grid_logreg.best_params_)
print("Confusion Matrix:\n", confusion_matrix(y_test, logreg_pred))
print("Classification Report:\n", classification_report(y_test, logreg_pred))

print("\nüìå Random Forest Results:")
rf_pred = grid_rf.predict(X_test)
print("Best Params:", grid_rf.best_params_)
print("Confusion Matrix:\n", confusion_matrix(y_test, rf_pred))
print("Classification Report:\n", classification_report(y_test, rf_pred))

"""# Pick better model based on recall"""

from sklearn.metrics import recall_score
recall_logreg = recall_score(y_test, logreg_pred)
recall_rf = recall_score(y_test, rf_pred)

if recall_rf > recall_logreg:
    print(f"\n‚úÖ Random Forest selected (Recall: {recall_rf:.2f})")
    best_model = grid_rf.best_estimator_
else:
    print(f"\n‚úÖ Logistic Regression selected (Recall: {recall_logreg:.2f})")
    best_model = grid_logreg.best_estimator_

# Save final model
joblib.dump(best_model, 'telco_churn_final_model.pkl')

# Predict on a sample
sample = X_test.iloc[[0]]
print("\nSample Prediction:", best_model.predict(sample))

import matplotlib.pyplot as plt

# STEP 1: Only do feature importance if best model is Logistic Regression
if isinstance(best_model.named_steps['classifier'], LogisticRegression):
    print("\nüîç Feature Importance for Logistic Regression:")

    # Get preprocessing and classifier objects
    preprocessor = best_model.named_steps['preprocessor']
    classifier = best_model.named_steps['classifier']

    # Get transformed feature names
    num_features = preprocessor.transformers_[0][2]
    cat_features = preprocessor.transformers_[1][1]['encoder'].get_feature_names_out(preprocessor.transformers_[1][2])
    all_features = np.concatenate([num_features, cat_features])

    # Get coefficients
    coef = classifier.coef_[0]

    # Combine features and coefficients
    feature_importance = pd.DataFrame({
        'Feature': all_features,
        'Coefficient': coef
    })

    # Sort by absolute impact
    feature_importance['AbsCoefficient'] = np.abs(feature_importance['Coefficient'])
    feature_importance = feature_importance.sort_values(by='AbsCoefficient', ascending=False).head(15)

"""# Plot"""

import matplotlib.pyplot as plt

# STEP 1: Only do feature importance if best model is Logistic Regression
if isinstance(best_model.named_steps['classifier'], LogisticRegression):
    print("\nüîç Feature Importance for Logistic Regression:")

    # Get preprocessing and classifier objects
    preprocessor = best_model.named_steps['preprocessor']
    classifier = best_model.named_steps['classifier']

    # Get transformed feature names
    num_features = preprocessor.transformers_[0][2]
    cat_features = preprocessor.transformers_[1][1]['encoder'].get_feature_names_out(preprocessor.transformers_[1][2])
    all_features = np.concatenate([num_features, cat_features])

    # Get coefficients
    coef = classifier.coef_[0]

    # Combine features and coefficients
    feature_importance = pd.DataFrame({
        'Feature': all_features,
        'Coefficient': coef
    })

    # Sort by absolute impact
    feature_importance['AbsCoefficient'] = np.abs(feature_importance['Coefficient'])
    feature_importance = feature_importance.sort_values(by='AbsCoefficient', ascending=False).head(15)

    plt.figure(figsize=(10, 6))
    plt.barh(feature_importance['Feature'], feature_importance['Coefficient'], color='skyblue')
    plt.xlabel('Coefficient Value')
    plt.title('Top 15 Most Influential Features on Churn (Logistic Regression)')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()

else:
    print("Feature importance plot is only available for Logistic Regression in this version.")